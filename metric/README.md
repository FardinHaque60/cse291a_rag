# Evaluation metrics

In this project, the metric we used are MRR, Precision@K, Recall@K, and nDCG@K.

to run the evaluation : 

```
python evaluation.py
```

To run the file, it need to have the label of id for the relevance chunk, relevance documents and retrieved results. 